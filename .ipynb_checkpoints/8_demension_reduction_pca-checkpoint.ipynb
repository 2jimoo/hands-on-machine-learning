{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72bdd965",
   "metadata": {},
   "source": [
    "# 데이터의 차원 \n",
    "- 차원 축소의 목표\n",
    "    - 훈련 속도, 데이터 시각화에 유용\n",
    "- 원리\n",
    "    - 거의 대부분 무관(일정)한 데이터 => 제거 대상\n",
    "    - 일부는 특성간 강한 연관성 => 표현 대상\n",
    "- 배경\n",
    "    - 고차원일수록 인접 두 샘플의 거리가 멀어진다.\n",
    "    - 따라서 데이터가 고차원일수록 많은 외삽이 일어나 과대적합됨\n",
    "- 개선 방법\n",
    "    - 훈련 샘플의 밀도가 충분히 높아질 때까지 데이터 추가(차원 대비 기하급수적으로 필요)\n",
    "    - 데이터 차원 축소\n",
    "\n",
    "# Principal Components Analysis\n",
    "- 공부 필요..\n",
    "https://github.com/rickiepark/handson-ml2/blob/master/08_dimensionality_reduction.ipynb\n",
    "\n",
    "- 개념\n",
    "    - 데이터에 가장 가까운 초평면을 찾아 투영하면 데이터의 차원을 줄일 수 있다.\n",
    "- 가정\n",
    "    - 데이터의 평균이 0이다.(pca 하기전 데이터를 원점에 맞춰야함)\n",
    "- 초평면 찾기\n",
    "    - 분산이 가장 큰 축(원본과 투영본 사이의 평균 제곱거리가 최소화되는 축)을 선택한다.\n",
    "    - 그 축에 직교하고 다음으로 분산이 큰 축을 선택한다.\n",
    "    - 데이터 셋 차원 수 n만큼 반복한다.\n",
    "    - d차원 초평면(주성분 n개 중 d개=공분산 행렬 첫 d열)에 투영하여 데이터 차원 축소가능\n",
    "- 초평면 빠르게 찾기\n",
    "    - 특잇값 분해 \n",
    "    - 공분산 행렬의 고유벡터=분산이 어느 방향으로 가장 큰 가=주성분\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2e83671",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3D 데이터셋을 만듭니다\n",
    "import numpy as np\n",
    "np.random.seed(4)\n",
    "m = 60\n",
    "w1, w2 = 0.1, 0.3\n",
    "noise = 0.1\n",
    "\n",
    "angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5\n",
    "X = np.empty((m, 3))\n",
    "X[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * np.random.randn(m) / 2\n",
    "X[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2\n",
    "X[:, 2] = X[:, 0] * w1 + X[:, 1] * w2 + noise * np.random.randn(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fe65acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#데이터셋에서 평균을 빼는 작업도 대신 처리해 줍니다\n",
    "# PCA변환기의 components_ 속성에 W^d 전치(계산된 주성분)가 담겨있습니다\n",
    "pca= PCA(n_components=2)\n",
    "X2D= pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5b3bbb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.84248607, 0.14631839])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 주선분의 축을 따라 있는 데이터셋의 분산 비율\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d114fae9",
   "metadata": {},
   "source": [
    "# 축소할 차원 수 고르기\n",
    "- 충분한 분산(ex 95%)가 될 때까지 차원수 늘리기\n",
    "- 설명된 분산을 차원수에 대한 함수로 그려 파악하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e7a0761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 보존하려는 분산의 비율 지정 가능\n",
    "pca = PCA(n_components=0.95)\n",
    "X_reduced= pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f14b6a",
   "metadata": {},
   "source": [
    "# 재구성\n",
    "- 재구성 오차: 원본과 재구성 데이터 사이의 평균 제곱 거리\n",
    "\n",
    "# 속도/메모리 개선\n",
    "- 랜덤 PCA\n",
    "    - 확률적 알고리즘을 사용해 처음 d개 주성분에 대한 근삿값을 빠르게 찾는다.\n",
    "- 점진적 PCA\n",
    "    - SVD알고리즘은 전체 훈련세트를 필요로함 IPCA는 추가되는 데이터에 대해 특정순간의 배열 일부만 사용해 학습(partial_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd683db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "mnist.target = mnist.target.astype(np.uint8)\n",
    "\n",
    "X = mnist[\"data\"]\n",
    "y = mnist[\"target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "#pca = PCA(n_components=154)\n",
    "#rnd_pca= PCA(n_components=154, svd_solver='randomized')\n",
    "#X_reduced= pca.fit_transform(X_train)\n",
    "#X_recovered= pca.inverse_transform(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88e43475",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "n_batches= 100\n",
    "inc_pca= IncrementalPCA(n_components=154)\n",
    "for X_batch in np.array_split(X_train, n_batches):\n",
    "    inc_pca.partial_fit(X_batch)\n",
    "    \n",
    "X_reduced= inc_pca.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce237c0",
   "metadata": {},
   "source": [
    "#  Projection - 커널 PCA\n",
    "- 5장 커널트릭\n",
    "    - 매우 높은 고차원공간(특성 공간)으로 암묵적으로 매핑하여 SVM의 비선형 분류/회귀 가능\n",
    "    - 고차원 특성공간에서의 선형 결정 경계 = 원본공간에서는 복잡한 비선형 결정경계\n",
    "- kPCA\n",
    "    - 동일한 성질을 이용한 비지도 학습\n",
    "    - 그리드 탐색으로 가장 성능이 좋은 커널과 하이퍼 파라미터 gamma를 찾는다.(best_params_)\n",
    "        - 재구성 원상의 오차를 최소화하는 커널과 하이퍼파라미터 선택\n",
    "- fit_inverse_transform \n",
    "    - 투영된 샘플=훈련세트, 원본 샘플=타깃세트로 하여 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba164869",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "clf= Pipeline([\n",
    "    (\"kpca\", KernelPCA(n_components=2)),\n",
    "    (\"log_reg\", LogisticRegression())\n",
    "])\n",
    "\n",
    "param_grid=[{\n",
    "    \"kpca__gamma\": np.linspace(0.03, 0.05, 10),\n",
    "    \"kpca__kernel\": [\"rbf\", \"sigmoid\"]\n",
    "}]\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=3)\n",
    "grid_search.fit(X,y)\n",
    "\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d0d261",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "rbf_pca=KernelPCA(n_components=2, kernel=\"rbf\",gamma=0.0433, fit_inverse_transform=True)\n",
    "X_reduced=rbf_pca.fit_transform(X)\n",
    "X_preimage= rbf_pca.inverse_transform(X_reduced)\n",
    "\n",
    "mean_squared_error(X_X_preimage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e880492",
   "metadata": {},
   "source": [
    "# Manifold - LLE\n",
    "- 지역 선형 임베딩(Locally linear embedding)\n",
    "    - 투영에 의존하지안흔ㄴ 매니폴드 학습법\n",
    "- 과정\n",
    "    - 각 훈련샘플에 가장 가까운 이웃들과의 선형관계(가중치) 측정\n",
    "    - 국부관계가 가장 잘 보존되는 훈련세트의 저차원 표현을 찾는다.\n",
    "        - d차원 공간에서 x_i의 상 p_i라면\n",
    "        - 선형관계를 적용하여 예측값과 오차가 최소화될 것이라는 가정\n",
    "- 원본 특성 차원수^2에 복잡도 비례하여 대용량 데이터에 적용하기 어려움"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c03d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "lle= LocallyLinearEmbedding(n_components=2, n_neighbors=10)\n",
    "X_reduced=lle.fit_transform(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
