{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72bdd965",
   "metadata": {},
   "source": [
    "# 데이터의 차원 \n",
    "- 차원 축소의 목표\n",
    "    - 훈련 속도, 데이터 시각화에 유용\n",
    "- 원리\n",
    "    - 거의 대부분 무관(일정)한 데이터 => 제거 대상\n",
    "    - 일부는 특성간 강한 연관성 => 표현 대상\n",
    "- 배경\n",
    "    - 고차원일수록 인접 두 샘플의 거리가 멀어진다.\n",
    "    - 따라서 데이터가 고차원일수록 많은 외삽이 일어나 과대적합됨\n",
    "- 개선 방법\n",
    "    - 훈련 샘플의 밀도가 충분히 높아질 때까지 데이터 추가(차원 대비 기하급수적으로 필요)\n",
    "    - 데이터 차원 축소\n",
    "\n",
    "# Principal Components Analysis\n",
    "- 공부 필요..\n",
    "https://github.com/rickiepark/handson-ml2/blob/master/08_dimensionality_reduction.ipynb\n",
    "\n",
    "- 개념\n",
    "    - 데이터에 가장 가까운 초평면을 찾아 투영하면 데이터의 차원을 줄일 수 있다.\n",
    "- 가정\n",
    "    - 데이터의 평균이 0이다.(pca 하기전 데이터를 원점에 맞춰야함)\n",
    "- 초평면 찾기\n",
    "    - 분산이 가장 큰 축(원본과 투영본 사이의 평균 제곱거리가 최소화되는 축)을 선택한다.\n",
    "    - 그 축에 직교하고 다음으로 분산이 큰 축을 선택한다.\n",
    "    - 데이터 셋 차원 수 n만큼 반복한다.\n",
    "    - d차원 초평면(주성분 n개 중 d개=공분산 행렬 첫 d열)에 투영하여 데이터 차원 축소가능\n",
    "- 초평면 빠르게 찾기\n",
    "    - 특잇값 분해 \n",
    "    - 공분산 행렬의 고유벡터=분산이 어느 방향으로 가장 큰 가=주성분\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2e83671",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3D 데이터셋을 만듭니다\n",
    "import numpy as np\n",
    "np.random.seed(4)\n",
    "m = 60\n",
    "w1, w2 = 0.1, 0.3\n",
    "noise = 0.1\n",
    "\n",
    "angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5\n",
    "X = np.empty((m, 3))\n",
    "X[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * np.random.randn(m) / 2\n",
    "X[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2\n",
    "X[:, 2] = X[:, 0] * w1 + X[:, 1] * w2 + noise * np.random.randn(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fe65acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#데이터셋에서 평균을 빼는 작업도 대신 처리해 줍니다\n",
    "# PCA변환기의 components_ 속성에 W^d 전치(계산된 주성분)가 담겨있습니다\n",
    "pca= PCA(n_components=2)\n",
    "X2D= pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5b3bbb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.84248607, 0.14631839])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 주선분의 축을 따라 있는 데이터셋의 분산 비율\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d114fae9",
   "metadata": {},
   "source": [
    "# 축소할 차원 수 고르기\n",
    "- 충분한 분산(ex 95%)가 될 때까지 차원수 늘리기\n",
    "- 설명된 분산을 차원수에 대한 함수로 그려 파악하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e7a0761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 보존하려는 분산의 비율 지정 가능\n",
    "pca = PCA(n_components=0.95)\n",
    "X_reduced= pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f14b6a",
   "metadata": {},
   "source": [
    "# 재구성\n",
    "- 재구성 오차: 원본과 재구성 데이터 사이의 평균 제곱 거리\n",
    "\n",
    "# 속도/메모리 개선\n",
    "- 랜덤 PCA\n",
    "    - 확률적 알고리즘을 사용해 처음 d개 주성분에 대한 근삿값을 빠르게 찾는다.\n",
    "- 점진적 PCA\n",
    "    - SVD알고리즘은 전체 훈련세트를 필요로함 IPCA는 추가되는 데이터에 대해 특정순간의 배열 일부만 사용해 학습(partial_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd683db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/datasets/_openml.py:932: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "mnist.target = mnist.target.astype(np.uint8)\n",
    "\n",
    "X = mnist[\"data\"]\n",
    "y = mnist[\"target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "pca = PCA(n_components=154)\n",
    "#rnd_pca= PCA(n_components=154, svd_solver='randomized')\n",
    "X_reduced= pca.fit_transform(X_train)\n",
    "X_recovered= pca.inverse_transform(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88e43475",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "n_batches= 100\n",
    "inc_pca= IncrementalPCA(n_components=154)\n",
    "for X_batch in np.array_split(X_train, n_batches):\n",
    "    inc_pca.partial_fit(X_batch)\n",
    "    \n",
    "X_reduced= inc_pca.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0594d26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
